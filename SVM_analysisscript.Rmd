---
title: "SVM Analysis"
output: html_document
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("~/UGA/Courses/Fall 2023/MIST 7635 Machine Learning Bus Analytics/Group Projects")
library(caret)
library(tidyverse)
library(VIM)
library(kernlab)
library(klaR)
library(performanceEstimation)
library(DMwR)
library(ROSE)
```

## Load data
```{r}
# Load the data
dat <- read.csv("wcgs.csv")
```

```{r}
# Imputing cholesterol within domain
dat_imp <- hotdeck(
dat,
variable = "chol",
domain_var = c("smoke","dibpat"),
ord_var = c("age","bmi"))

# examine the imputed cholesterol value  
dat_imp[is.na(dat$chol),"chol"]

# Imputing arcus within domain
dat_imp <- hotdeck(
dat_imp,
variable = "arcus",
domain_var = c("smoke","dibpat"),
ord_var = c("age","chol"))

# examine the imputed arcus value  
dat_imp[is.na(dat$arcus),"arcus"]

```

#### Split data
```{r}
set.seed(123)
# create train/test split
idx = createDataPartition(dat_imp$chd69, p=0.75, list=F)
train.chd69 = dat_imp[idx,]
test.chd69 = dat_imp[-idx,]
```

#### Implement over- and under-sampling for imbalanced data
```{r}
tain_data_balanced_over <- ovun.sample(factor(chd69) ~ age+bmi+lnsbp+dbp+chol+factor(dibpat)+factor(smoke)+factor(arcus),  
                                  data = train.chd69, 
                                  method = "over",N=4346, seed = 1)$data
table(tain_data_balanced_over$chd69)

```

#### Logistic regression
```{r}
model <- glm(factor(chd69) ~ age+bmi+lnsbp+dbp+chol+factor(dibpat)+factor(smoke)+factor(arcus), data = tain_data_balanced_both, family = binomial)
summary(model)

pred = predict(model, newdata = test.chd69, type="response")

y_test = test.chd69$chd69
y = ifelse(pred>0.5, 1, 0)

# Confusion matrix
table(y, y_test)

(497+46)/3154

```

#### SVM linear classifier
```{r}
set.seed(123)
train_control = trainControl(method = "repeatedcv", number = 5, repeats = 5, savePred=T, classProb=T)
x = train.chd69[,c("age","bmi","lnsbp","dbp","chol","dibpat","smoke","arcus")]
y = train.chd69$chd69

# Fit the model 
svm1 <- train(x,y, 
      
              method = "nb", 
              trControl = train_control,  
              preProcess = c("center","scale"))
#View the model
svm1
svm1$result
#accuracy:0.9184288



pred = predict(model, newdata = test.chd69, type="response")

y_test = test.chd69$chd69
y = ifelse(pred>0.5, 1, 0)

# Confusion matrix
table(y, y_test)

(497+46)/3154


```


#### SVM linear classifier
```{r}

# Fit the model 
svm1 <- train(factor(chd69) ~ age+bmi+lnsbp+dbp+chol+factor(dibpat)+factor(smoke)+factor(arcus), data = tain_data_balanced_over, method = "svmLinear", trControl = train_control, preProcess = c("center","scale"))
# Print the best tuning parameter sigma and C that maximizes model accuracy
svm1
svm1$bestTune
#accuracy:0.9184288



```

## Tune the hyperparameter Cost
```{r}
# Fit the model 
tunegrid=data.frame(C=c(0.1, 1, 10, 100))

svm2 <- train(factor(chd69) ~ age+bmi+lnsbp+dbp+chol+factor(dibpat)+factor(smoke)+factor(arcus), data = train.chd69, method = "svmLinear", trControl = train_control,  preProcess = c("center","scale"), tuneGrid = tunegrid) # might just use the default
#View the model
svm2
```

#### SVM Polynomial classifier
```{r}
# Fit the model 
svm3 <- train(factor(chd69) ~ age+bmi+lnsbp+dbp+chol+factor(dibpat)+factor(smoke)+factor(arcus), data = train.chd69, method = "svmPoly", trControl = train_control, preProcess = c("center","scale"))
# Print the best tuning parameter sigma and C that maximizes model accuracy
svm3
svm3$bestTune
#accuracy:0.9184288
res3 <- as_tibble(svm3$results[which.min(svm3$results[,2]),])
res3

```

#### SVM Radical classifier
```{r}
# Fit the model 
svm4 <- train(factor(chd69) ~ age+bmi+lnsbp+dbp+chol+factor(dibpat)+factor(smoke)+factor(arcus), data = train.chd69, method = "svmRadial", trControl = train_control, preProcess = c("center","scale"))
# Print the best tuning parameter sigma and C that maximizes model accuracy
svm4$results
svm4$bestTune
#accuracy:0.9184286	
res4 <- as_tibble(svm4$results[which.min(svm4$results[,2]),])
res4
```

#### Performance evaluation on testing set
```{r}
pred1 = predict(svm1, newdata = test.chd69, type="prob")
pred3 = predict(model_qda,test)$posterior[,"1"]
pred4 = predict(model_nb,test,type="raw")[,"1"]

y_test = test.chd69$chd69
y_1 = ifelse(pred1[,2]>0.5, 1, 0)
y_3 = ifelse(pred3[,2]>0.5, 1, 0)
y_4 = ifelse(pred4[,2]>0.5, 1, 0)

# Confusion matrix
table(y_1, y_test)
table(y_2, y_test)
table(y_3, y_test)

# Accuracy
df<-tibble(Model=c('SVM Linear','SVM Poly','SVM Radical'),
           Accuracy=c(svm1$results[2][[1]],res3$Accuracy,res4$Accuracy))
df %>% arrange(Accuracy)

# ROC


```


